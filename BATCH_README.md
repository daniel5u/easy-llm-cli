# 批量任务处理系统

这是一个基于 easy-llm-cli 的批量任务处理系统，可以自动执行多个AI任务并监控其状态，包括详细的token使用统计和成本分析。支持串行和并行两种执行模式。

## 文件结构

- `test.js` - 核心任务执行模块（已优化，减少console输出，新增token统计）
- `batch.js` - 串行批量处理器
- `parallel-batch.js` - 并行批量处理器
- `task.json` - 任务配置文件
- `test-batch.js` - 测试脚本
- `token-calculator.js` - Token成本计算器
- `run-all-tasks.js` - 交互式运行脚本
- `BATCH_README.md` - 本文档

## 使用方法

### 1. 配置任务

在 `task.json` 中配置你的任务：

```json
{
  "tasks": [
    {
      "dir": "",
      "first_prompt": "用 Python 写一个猜数字游戏"
    },
    {
      "dir": "",
      "first_prompt": "用 HTML+JS 写一个计算器"
    }
  ]
}
```

### 2. 运行批量处理

```bash
# 交互式选择执行模式
node run-all-tasks.js

# 直接并行执行所有任务
node parallel-batch.js

# 直接串行执行所有任务
node batch.js

# 测试单个任务
node test-batch.js

# 查看token成本计算器示例
node token-calculator.js
```

### 3. 查看结果

执行完成后，结果会保存在以下文件中：
- `batch_results.json` - 串行执行结果
- `parallel_batch_results.json` - 并行执行结果
- `test_results.json` - 测试模式结果

包含内容：
- 执行摘要（成功率、耗时等）
- 每个任务的详细结果
- Token使用统计
- 成本分析
- 错误信息和状态

## 功能特性

### ✅ 已实现的功能

1. **批量执行**: 自动读取 `task.json` 中的所有任务
2. **并行处理**: 支持同时执行多个任务，大幅提升效率
3. **状态监控**: 实时显示每个任务的执行状态
4. **Token统计**: 详细记录每个任务的token使用情况
5. **成本分析**: 自动计算API调用成本，支持多模型对比
6. **错误处理**: 捕获并记录执行错误
7. **结果保存**: 自动保存执行结果到JSON文件
8. **进度显示**: 显示当前进度和剩余任务
9. **超时保护**: 防止任务无限循环（最多10轮迭代）
10. **API限制保护**: 任务间自动休息，避免API限制

### 📊 状态监控

系统会实时显示以下状态：
- `开始处理初始任务...`
- `第 X 轮评估中...`
- `任务已完成`
- `错误: [错误信息]`
- `达到最大迭代次数，任务未完成`

### 🎯 执行结果

每个任务会返回以下状态之一：
- `completed` - 任务成功完成
- `error` - 执行过程中出现错误
- `timeout` - 达到最大迭代次数但未完成

### 💰 Token统计和成本分析

系统会自动统计：
- **总Token使用量**: 包括输入和输出token
- **API调用次数**: 每次任务的调用频率
- **成本估算**: 基于不同模型的定价计算
- **成本对比**: 显示不同模型间的成本差异

## 执行模式

### 1. 串行执行 (batch.js)
- 一个任务完成后才开始下一个
- 适合API限制较严格的情况
- 内存使用较少
- 执行时间较长

### 2. 并行执行 (parallel-batch.js)
- 同时执行多个任务
- 大幅提升执行效率
- 可配置并发数量
- 适合大量任务处理

### 3. 交互式执行 (run-all-tasks.js)
- 提供友好的命令行界面
- 可选择执行模式
- 支持测试模式（只执行前3个任务）

## 配置说明

### task.json 格式

```json
{
  "tasks": [
    {
      "dir": "工作目录路径（可选，默认为当前目录）",
      "first_prompt": "任务描述"
    }
  ]
}
```

### 环境要求

确保已安装必要的依赖：
```bash
npm install openai easy-llm-cli
```

## 示例输出

### 并行执行示例
```
✅ 已加载 10 个任务

🎯 开始并行执行 10 个任务 (最大并发: 3)
============================================================

🚀 开始执行任务 1/10
📝 任务内容: 用 Python 写一个 30 行内的猜数字游戏：电脑随机生成 1~10 的整数...
📁 工作目录: /Users/username/project
[14:30:15] 任务 1/10: 开始处理初始任务...
[14:30:45] 任务 1/10: 第 1 轮评估中...
[14:31:10] 任务 1/10: 任务已完成
✅ 任务 1 完成 (耗时: 55.23s) [1/10]
📊 状态: completed, 迭代次数: 2
💾 📊 Token使用: 总计2,450 (输入1,200, 输出1,250) - 3次调用
💰 成本: $0.0007 (输入: $0.0002), 输出: $0.0005)

🚀 开始执行任务 2/10
📝 任务内容: 用 HTML+JS 写一个 50 行内的加法计算器：两个输入框、一个"="按钮...
📁 工作目录: /Users/username/project
[14:30:20] 任务 2/10: 开始处理初始任务...
[14:31:05] 任务 2/10: 任务已完成
✅ 任务 2 完成 (耗时: 45.12s) [2/10]
📊 状态: completed, 迭代次数: 1
💾 📊 Token使用: 总计1,890 (输入950, 输出940) - 2次调用
💰 成本: $0.0005 (输入: $0.0001), 输出: $0.0003)

============================================================
🎉 并行执行完成! 总耗时: 120.45s

📊 执行摘要:
✅ 成功完成: 8 个任务
❌ 执行失败: 1 个任务
⏰ 超时未完成: 1 个任务
📈 成功率: 80.0%

💾 Token使用统计:
📊 总Token: 15,234
📥 输入Token: 7,890
📤 输出Token: 7,344
🔄 API调用次数: 25
📊 平均每次调用: 609 tokens

📋 详细结果:
✅ 任务 1: completed (55.23s) (2,450 tokens)
✅ 任务 2: completed (45.12s) (1,890 tokens)
❌ 任务 3: error (12.34s)
⏰ 任务 4: timeout (180.00s)
...
```

## 性能对比

### 执行时间对比
- **串行执行**: 10个任务 × 60秒 = 600秒 (10分钟)
- **并行执行**: 10个任务 ÷ 3并发 = 200秒 (3.3分钟)
- **效率提升**: 约67%的时间节省

### 资源使用对比
- **串行执行**: 内存使用稳定，API调用有序
- **并行执行**: 内存使用较高，API调用并发
- **推荐配置**: 3-5个并发数，平衡效率和稳定性

## 成本分析

### 支持的模型

系统内置了常见AI模型的定价信息：
- **DeepSeek Chat**: 最经济的选择
- **GPT-4**: 高质量但成本较高
- **GPT-3.5 Turbo**: 平衡性能和成本
- **Claude-3 Opus**: 高质量但成本高
- **Claude-3 Sonnet**: 平衡选择

### 成本优化建议

1. **选择合适的模型**: 根据任务复杂度选择性价比最高的模型
2. **监控Token使用**: 关注输入/输出token比例，优化提示词
3. **批量处理**: 利用批量处理减少API调用开销
4. **错误处理**: 避免重复执行失败的任务
5. **并发控制**: 根据API限制调整并发数量

## 故障排除

### 常见问题

1. **任务加载失败**
   - 检查 `task.json` 文件是否存在且格式正确
   - 确保JSON语法正确

2. **API错误**
   - 检查网络连接
   - 验证API密钥是否有效
   - 检查API配额是否充足
   - 降低并发数量

3. **任务超时**
   - 任务可能过于复杂，需要更多轮次
   - 可以调整 `maxIterations` 参数

4. **Token统计不准确**
   - 某些API可能不返回token使用信息
   - 检查API响应格式是否包含usage字段

5. **并行执行问题**
   - 降低并发数量
   - 检查API限制
   - 增加任务间休息时间

### 调试模式

如果需要更详细的日志，可以修改 `test.js` 中的 `log` 参数：
```javascript
log: true, // 改为 true 启用详细日志
```

## 扩展功能

### 自定义配置

可以修改以下参数：
- `maxIterations`: 最大迭代次数（默认10）
- `maxConcurrent`: 最大并发数（默认3）
- 任务间休息时间（默认2000ms）
- 输出文件路径

### 添加新功能

系统设计为模块化，可以轻松添加：
- 更智能的并发控制
- 更详细的状态报告
- 邮件通知功能
- 自定义结果格式
- 更多模型支持
- 任务优先级管理

## 注意事项

1. **API成本**: 批量执行会消耗大量API调用，请注意成本控制
2. **网络稳定性**: 确保网络连接稳定，避免任务中断
3. **资源管理**: 长时间运行可能消耗较多内存，建议定期重启
4. **结果备份**: 重要结果建议定期备份到其他位置
5. **Token限制**: 注意不同模型的token限制，避免超出配额
6. **并发控制**: 根据API限制调整并发数量，避免触发限制
7. **错误恢复**: 系统会自动处理失败的任务，但建议监控执行状态 